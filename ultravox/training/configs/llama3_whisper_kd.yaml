# SLM with ultravox & llama3, trained wtih knowledge distillation.
exp_name: "llama3_whisper_s"

# Make sure to accept the license agreement on huggingface hub
text_model: "meta-llama/Meta-Llama-3-8B-Instruct"
audio_model: "openai/whisper-small"


loss_config:
  # Choose from ["KL_Divergence", "CrossEntropy"], default is "KL_Divergence"
  loss_function: "KL_Divergence"

# Temporarily remove heysquad_human from val_sets as it causes the training to fail.
val_sets: ["anyinstruct", "soda", "peoplespeech"]

# Load continuation training data using GenericVoiceDataset.
# Select 100,000 samples from both clean and other subsets.
# Mix clean and other subsets in a 2:1 ratio for batch creation.
# Total training steps: 4000, with each step 4x8=32 samples.
batch_size: 4
max_steps: 1000

data_sets: ["gigaspeech", "anyinstruct", "soda"]
