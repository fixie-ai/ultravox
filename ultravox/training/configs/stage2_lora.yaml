exp_name: stage2_lora__gs_ai_bq_soda

text_model_lora_config:
  r: 64  # no/little change in the range [16, 64]
  target_modules: ['mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj', 'v_proj', 'o_proj', 'k_proj', 'q_proj']

data_sets: ["gigaspeech", "anyinstruct", "boolq_extended", "soda"]

# disable_layer_drop: True
# audio_model_lora_config:
#   r: 64
#   target_modules: ['k_proj', 'q_proj', 'v_proj', 'out_proj', 'intermediate_dense', 'output_dense']

num_prompts: 11

lr: 1.e-4  # need a lower LR for LLM fine-tuning
lr_scheduler: constant_with_warmup
lr_warmup_steps: 250
max_steps: 5_000
save_steps: 0

batch_size: 2
grad_accum_steps: 2
